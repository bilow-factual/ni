Hadoop operator.
The entry point for running various kinds of Hadoop jobs.

c
BEGIN {defshort '/H', defdsp 'hadoopalt', 'hadoop job dispatch table'}

Streaming.
We need to be able to find the Streaming jar, which is slightly nontrivial. The
hadoop docs suggest that $HADOOP_HOME has something to do with it, but I've
seen installations that had no such environment variable and everything worked
fine. Here's what we can do:

| 1. Use $NI_HADOOP_STREAMING_JAR if it's set
  2. Use `locate hadoop-streaming*.jar` if we have `locate`
  3. Use `find /usr /opt -name hadoop-streaming*.jar`, see if it's there
  4. Use `find / -name hadoop-streaming*.jar`, hope it's there

If those don't work, then we are officially SOL and you'll have to set
NI_HADOOP_STREAMING_JAR.

sub hadoop_streaming_jar {
  local $SIG{CHLD} = 'DEFAULT';
  $ENV{NI_HADOOP_STREAMING_JAR}
  || (split /\n/, `locate 'hadoop-streaming*.jar' \\
                   || find /usr    -name 'hadoop-streaming*.jar' \\
                   || find /opt    -name 'hadoop-streaming*.jar' \\
                   || find / -xdev -name 'hadoop-streaming*.jar'`)[0]
  || die "ni: cannot find hadoop streaming jar "
       . "(you can fix this by setting \$NI_HADOOP_STREAMING_JAR)";
}

Input type autodetection.
Technically, hadoop operators take one or more HFDS input paths on stdin -- but
of course ni isn't going to just give up if we appear to have something else.
If we have something that obviously isn't an HDFS path, we upload that stream
into a temporary HDFS location and run against that.

sub hdfs_input_path {
  local $_;
  my $n;
  die "ni: hdfs_input_path: no data" unless $n = saferead \*STDIN, $_, 8192;
  if (/^hdfs:\/\//) {
    $n = saferead \*STDIN, $_, 8192, length while $n;
    grep length, split /\n/;
  } else {
    my $hdfs_tmp    = resource_tmp 'hdfs://';
    my $hdfs_writer = resource_write $hdfs_tmp;
    safewrite $hdfs_writer, $_;
    safewrite $hdfs_writer, $_ while saferead \*STDIN, $_, 8192;
    close $hdfs_writer;
    $hdfs_writer->await;
    $hdfs_tmp;
  }
}

sub hadoop_lambda_file($$) {
  my ($name, $lambda) = @_;
  my $tmp = resource_tmp('file://') . $name;
  my $w   = resource_writer $tmp;
  my ($stdin, @exec) = sni_exec_list @$lambda;
  safewrite $w, $stdin;
  close $w;
  $exec[1] eq '-'
    ? ($exec[1] = $tmp) =~ s/^.*\///
    : die "ni: sni_exec_list has produced @exec; hadoop_lambda_file expected "
        . "- as the second argument in this list (the goal is to change the "
        . "command to have perl read its input from a file rather than stdin)";
  ($tmp, @exec);
}

defoperator hadoop_streaming => q{
  my ($map, $combine, $reduce) = @_;
  my $ipath = hdfs_input_path;
  my $opath = resource_tmp "hdfs://";
  my ($mapper, @map_cmd) = hadoop_lambda_file 'mapper', @$map;
  my ($combiner, @combine_cmd) = defined $combine
    ? hadoop_lambda_file 'combiner', @$combine : ();
  my ($reducer, @reduce_cmd) = defined $reduce
    ? hadoop_lambda_file 'reducer', @$reduce : ();

  my $streaming_jar = hadoop_streaming_jar;

  my $hadoop_fh = siproc {
    $mapper   =~ s|^file://||;
    $combiner =~ s|^file://|| if defined $combiner;
    $reducer  =~ s|^file://|| if defined $reducer;
    exec 'hadoop', 'jar', $streaming_jar,
         '-D', "mapred.job.name=ni $ipath -> $opath",
         '-input', $ipath,
         '-output', $opath,
         '-file',   $mapper,
         '-mapper', shell_quote(@map_cmd),
         (defined $combiner
           ? ('-file', $combiner, '-combiner', shell_quote @combine_cmd)
           : ()),
         (defined $reducer
           ? ('-file', $reducer, '-reducer', shell_quote @reduce_cmd)
           : ());
  };

  close $hadoop_fh;
  die "ni: hadoop streaming failed" if $hadoop_fh->await;

  resource_nuke $mapper;
  resource_nuke $combiner if defined $combiner;
  resource_nuke $reducer  if defined $reducer;

  print $opath, "\n";
};
