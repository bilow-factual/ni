Command-line option parser.
A context-aware command line parser, which in a Canard-powered world works as
the reader. Certain static symbols, despite being resolved at runtime, have
read-time parse semantics that make it possible for ni syntax to be as
expressive as (and often much more than) nfu. See design/cli.md for details.

package ni;

A parse state is just a modified copy of @ARGV, including string
transformations. For example:

| $ ni foo m'r a + b' T4
  # initial parse state is ("foo", "mr a + b", "T4")
  # quasifile parse step consumes "foo", so we then have ("mr a + b", "T4")
  # quasifile parse step rejects ("mr a + b", "T4") by returning ()
  # long option parsers all reject ("mr a + b", "T4")
  # short option parser "m" happens:
  #   m -> ruby-code
  #   ruby-code is run on ("r a + b", "T4") and returns (code, "T4")
  #     (had the code ended in extra ] characters, the parser would have
  #      returned those separately, e.g. (code, "]", "T4"))
  # ...

This might seem like it would be slow, but most of the overhead lives in
high-throughput native functions that are unlikely to take up much time in
practice.

use constant end_of_argv  => sub {@_           ? () : (0)};
use constant consumed_opt => sub {length $_[0] ? () : @_};

sub seqr(\@) {my ($ps) = @_;
         sub {my ($x, @xs, @ys, @ps);
              (($x, @_) = &$_(@_)) ? push @xs, $x : return () for @ps = @$ps;
              (\@xs, @_)}}

sub altr(\@) {my ($ps) = @_;
         sub {my @ps, @r; @r = &$_(@_) and return @r for @ps = @$ps; ()}}

sub seq(@) {seqr @_}
sub alt(@) {altr @_}

sub rep($;$) {my ($p, $n) = (@_, 0);
         sub {my (@c, @r);
              push @r, $_ while ($_, @_) = &$p(@c = @_);
              @r >= $n ? (\@r, @c) : ()}}

sub maybe($) {my ($p) = @_;
         sub {my @xs = &$p(@_); @xs ? @xs : (undef, @_)}};

sub pmap(&$) {my ($f, $p) = @_;
         sub {my @xs = &$p(@_); @xs ? (&$f($_ = $xs[0]), @xs[1..$#xs]) : ()}}

sub pif(&$) {my ($f, $p) = @_;
        sub {my @xs = &$p(@_); @xs && &$f($_ = $xs[0]) ? @xs : ()}}

sub ptag(@) {my (@xs, $p) = @_; $p = pop @xs; pmap {[@xs, $_]} $p}
sub pn($@)  {my ($n, @ps) = @_; pmap {$$_[$n]} seq @ps}

Match/consume regex.
Consumes the match, returning either the matched text or the first match group
you specify. Always matches from the beginning of a string.

sub mr($) {my $r = qr/$_[0]/;
      sub {my ($x, @xs) = @_; $x =~ s/^($r)// ? ($2 || $1, $x, @xs) : ()}}

sub mrc($) {pn 0, mr $_[0], maybe consumed_opt}

Character dispatch.
This is just a way to bypass a lot of the alt() overhead that would otherwise
result to decode a high-entropy stream of text. The most obvious case is short
option parsing.

| chalt(a => seq(...), b => ..., ...)
  # functionally the same as alt(pn(1, mr('^a', seq(...))),
  #                              pn(1, mr('^b', ...)),
  #                              ...)

Note that the dispatch character itself isn't encoded into the result.

sub chaltr(\%) {my ($ps) = @_;
           sub {my ($x, @xs, $k, @ys, %ls) = @_;
                ++$ls{length $_} for keys %$ps;
                for my $l (sort {$b <=> $a} keys %ls) {
                  return (@ys = $$ps{$c}(substr($x, $l), @xs))
                    ? ($ys[0], @ys[1..$#ys])
                    : ()
                  if exists $$ps{$c = substr $x, 0, $l};
                }
                ()}}

sub chalt(%) {my %h = @_; chaltr %h}

Regex parsing.
Sometimes we'll have an operator that takes a regex, which is subject to the
CLI reader problem the same way code arguments are. Rather than try to infer
brackets the same way, we just require that regexes are terminated with /
(which should be ok because that's also how they typically start).

use constant regex => pmap {s/\/$//; $_} mr '^(?:[^\\/]|\\.)*/';

Code parsing.
This is nontrivial due to the CLI reader problem. The idea is that we need to
figure out how many closing brackets belong to the code, vs how many close a
lambda. Depending on the language, the only way to do this may be to shell out
to an interpreter.

use constant rbcode => sub {
  return @_ unless $_[0] =~ /\]$/;
  my ($code, @xs, $x, $qcode) = @_;
  ($qcode = $code) =~ s/'/'\\''/g;
  $x .= ']' while $_ = system("ruby -ce '$qcode' >/dev/null 2>&1")
                  and ($qcode =~ s/\]$//, $code =~ s/\]$//);
  $_ ? () : length $x ? ($code, $x, @xs) : ($code, @xs)};

Perl code is similar to Ruby, but we need to explicitly disable any BEGIN{}
blocks to avoid executing side effects. We can guarantee that nothing will run
(beyond `use` statements, which we assume are safe) by removing any
occurrences of the string `BEGIN` and replacing them with something
syntactically equivalent but less volatile -- in this case, `END`.

use constant plcode => sub {
  return @_ unless $_[0] =~ /\]$/;
  my ($code, @xs, $x, $qcode) = @_;
  ($qcode = $code) =~ s/'/'\\''/g;

  my $begin_warning = $qcode =~ s/BEGIN/END/g;
  $x .= ']' while $_ = system("perl -ce '$qcode' >/dev/null 2>&1")
                  and ($qcode =~ s/\]$//, $code =~ s/\]$//);

  print STDERR <<EOF if $_ && $begin_warning;
ni: failed to get closing bracket count for perl code "$_[0]", possibly
    because BEGIN-block metaprogramming is disabled when ni tries to figure
    this out.
    https://github.com/spencertipping/ni/tree/master/design/cli-reader-problem.md
EOF
  $_ ? () : length $x ? ($code, $x, @xs) : ($code, @xs)};

CLI option parsing.
%syntax_elements is an alias table so we can describe operator syntax using
just strings. %operator_syntax maps short operator letters to a parser for
their arguments (if any); for example:

| use constant takespec => mr '^\d+|^\+\d+';
  $operators{T} = psh 'head', pmap {("-m", $_)} takespec;

Quasifiles are parsed with early preference using subs from
@quasifile_parsers; functionally it's interpreted as a choice that happens
_before_ option parsing happens. This means that in the event of something
ambiguous, e.g. `ni f00`, where `f00` is the name of a file, the file
interpretation will be preferred. If you want to force the option
interpretation, you need to say `ni -f00` (provided that `-f00` isn't itself a
file).

our %short;
our @long;

sub defshort($$) {$short{$_[0]} = $_[1]}
sub deflong($$)  {unshift @long, $_[1]}

CLI syntax elements.
Building blocks for ni's command-line grammar and the toplevel parser. Here's
the big picture, which is a lot more subtle than nfu (and most command-line
tools, for that matter).

Notationally, ni supports a lot of interesting elements like anonymous
streams. nfu used whitespace-delimited square brackets for this and ended up
preprocessing them into shell commands, whereas ni keeps them as a first-class
"list" data type -- which means operators are at liberty to interpret them
however they'd like (with the understanding that ops and quasifiles are still
parsed as usual).

In nfu, there were two ways to write these streams:

| $ nfu --pipe [ foo ]  # compiled to nfu --pipe "nfu foo"
  $ nfu @[ foo ]        # compiled to nfu sh:"nfu foo"

This is suboptimal, of course, because it involves writing @ and knowing when
to use it. ni unifies the two:

| $ ni -X[foo]          # [foo] is a lambda command, will have stdin
  $ ni [foo]            # [foo] is a data source

In ni terms, "lambda" implies that something is taking input (e.g. a hadoop
mapper), whereas "source" implies that it's a self-sufficient producer of
data.

I define the toplevel parser to be self-referential for the moment. Later when
we `use constant` the outer function will be replaced with the real one,
resulting in the inner `ops(@_)` call pointing to it. (The indirection is
required, unfortunately; otherwise we'd be unable to change the reference later
from underneath the `use constant` expression.)

sub ops() {sub {ops()->(@_)}}

use constant long   => altr @long;
use constant short  => chaltr %short;
use constant lambda => alt mr '_', pn 1, mrc '\[', ops, mr '\]';
use constant thing  => alt lambda, long, short;
use constant suffix => rep thing, 1;
use constant op     => pn 1, rep(consumed_opt), thing, rep(consumed_opt);
use constant ops    => rep op;
use constant cli    => pn 0, ops, end_of_argv;
use constant cli_d  => ops;
